import requests
import json
import re
import time
from functools import lru_cache
from typing import Dict, List, Optional, Tuple, Any, Callable, Union # Union eklendi

# Ä°leriye dÃ¶nÃ¼k bildirim / Type hinting
if False:
    from aybarcore import EnhancedAybar


class LLMManager:
    """
    TÃ¼m LLM (BÃ¼yÃ¼k Dil Modeli) iletiÅŸimini yÃ¶netir.
    Fonksiyon Ã§aÄŸÄ±rma, prompt oluÅŸturma ve yanÄ±t ayrÄ±ÅŸtÄ±rma gibi gÃ¶revleri merkezileÅŸtirir.
    """
    def __init__(self, config_data: Dict, aybar_instance: "EnhancedAybar"):
        self.config_data = config_data
        self.aybar = aybar_instance # DiÄŸer sistemlere eriÅŸim iÃ§in (Ã¶rn: etik kontrol)

        self.api_url = self.config_data.get("LLM_API_URL", "http://localhost:1234/v1/completions")
        self.default_model_name = self.config_data.get("THINKER_MODEL_NAME", "mistral-7b-instruct-v0.2")
        self.default_max_tokens = self.config_data.get("MAX_TOKENS", 4096)
        self.default_timeout = self.config_data.get("TIMEOUT", 600) # saniye cinsinden

        self._last_error_time = 0
        self._error_cooldown = self.config_data.get("LLM_ERROR_COOLDOWN_SECONDS", 60)
        self._max_retry_attempts = self.config_data.get("LLM_MAX_RETRY_ATTEMPTS", 3)

    def _get_headers(self) -> Dict[str, str]:
        return {"Content-Type": "application/json"}

    def ask_llm(self,
                prompt_or_messages: Union[str, List[Dict[str, str]]],
                model_name: Optional[str] = None,
                max_tokens: Optional[int] = None,
                temperature: float = 0.5,
                **kwargs: Any
                ) -> str:
        """
        LLM'ye sorgu gÃ¶nderir ve metin yanÄ±tÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
        Hata durumunda veya cooldown aktifse uygun bir mesaj dÃ¶ndÃ¼rÃ¼r.
        """
        if time.time() - self._last_error_time < self._error_cooldown:
            return "âš ï¸ LLM Hata Cooldown: KÄ±sa bir sÃ¼re Ã¶nce bir hata oluÅŸtu, tekrar denemeden Ã¶nce bekleniyor."

        payload: Dict[str, Any] = {
            "max_tokens": max_tokens or self.default_max_tokens,
            "temperature": temperature,
            **kwargs # Ekstra parametreleri payload'a ekle
        }

        if isinstance(prompt_or_messages, str):
            payload["prompt"] = prompt_or_messages
        elif isinstance(prompt_or_messages, list):
            payload["messages"] = prompt_or_messages
        else:
            return "âš ï¸ LLM HatasÄ±: GeÃ§ersiz prompt/mesaj formatÄ±."

        payload["model"] = model_name or self.default_model_name
        # BazÄ± sunucular (Ã¶rn: llama.cpp server) 'model' parametresini desteklemez,
        # eÄŸer Ã¶yle bir durum varsa bu satÄ±r kaldÄ±rÄ±labilir veya ayarlanabilir.

        for attempt in range(self._max_retry_attempts):
            try:
                response = requests.post(
                    self.api_url,
                    headers=self._get_headers(),
                    json=payload,
                    timeout=self.default_timeout
                )
                response.raise_for_status() # HTTP hatalarÄ± iÃ§in exception fÄ±rlatÄ±r (4xx, 5xx)

                json_response = response.json()

                # YanÄ±t formatÄ±nÄ± kontrol et (OpenAI benzeri ve diÄŸerleri iÃ§in)
                if "choices" in json_response and isinstance(json_response["choices"], list) and json_response["choices"]:
                    first_choice = json_response["choices"][0]
                    if "text" in first_choice: # Tamamlama endpoint'i iÃ§in
                        return first_choice["text"].strip()
                    elif "message" in first_choice and "content" in first_choice["message"]: # Chat endpoint'i iÃ§in
                        return first_choice["message"]["content"].strip()
                elif "content" in json_response: # Basit metin yanÄ±tÄ± (bazÄ± llama.cpp modlarÄ±)
                     return json_response["content"].strip()

                self._log_llm_error(f"Bilinmeyen LLM yanÄ±t formatÄ±: {str(json_response)[:500]}", payload)
                return f"âš ï¸ LLM Format HatasÄ±: YanÄ±t formatÄ± anlaÅŸÄ±lamadÄ±."

            except requests.exceptions.Timeout:
                self._log_llm_error(f"Timeout (deneme {attempt + 1}/{self._max_retry_attempts})", payload)
                if attempt == self._max_retry_attempts - 1:
                    self._last_error_time = time.time()
                    return "âš ï¸ LLM BaÄŸlantÄ± HatasÄ±: Zaman aÅŸÄ±mÄ±."
            except requests.exceptions.RequestException as e:
                self._log_llm_error(f"RequestException (deneme {attempt + 1}/{self._max_retry_attempts}): {e}", payload)
                if attempt == self._max_retry_attempts - 1:
                    self._last_error_time = time.time()
                    return f"âš ï¸ LLM BaÄŸlantÄ± HatasÄ±: {e}"
            except json.JSONDecodeError as e:
                 self._log_llm_error(f"JSONDecodeError (deneme {attempt + 1}/{self._max_retry_attempts}): YanÄ±t JSON deÄŸil. YanÄ±t: {response.text[:200]}", payload)
                 if attempt == self._max_retry_attempts - 1:
                    self._last_error_time = time.time()
                    return f"âš ï¸ LLM YanÄ±t HatasÄ±: Sunucudan gelen yanÄ±t JSON formatÄ±nda deÄŸil."
            except Exception as e: # DiÄŸer beklenmedik hatalar
                self._log_llm_error(f"Genel Hata (deneme {attempt + 1}/{self._max_retry_attempts}): {type(e).__name__} - {e}", payload)
                if attempt == self._max_retry_attempts - 1:
                    self._last_error_time = time.time()
                    return f"âš ï¸ LLM Genel HatasÄ±: {type(e).__name__} - {e}"

            time.sleep(2 ** attempt) # Exponential backoff

        return "âš ï¸ LLM HatasÄ±: Maksimum yeniden deneme sayÄ±sÄ±na ulaÅŸÄ±ldÄ±."


    def ask_llm_with_function_calling(
        self,
        messages: List[Dict[str, str]],
        tools: Dict[str, Callable], # AraÃ§lar: {'tool_name': function_reference}
        model_name: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: float = 0.5,
        max_recursion_depth: Optional[int] = None,
        current_recursion_depth: int = 0
    ) -> Tuple[str, Optional[List[Dict[str, Any]]]]:
        """
        LLM'ye mesajlarÄ± gÃ¶nderir, fonksiyon Ã§aÄŸÄ±rma (araÃ§ kullanma) yeteneÄŸini kullanÄ±r.
        Gerekirse araÃ§larÄ± Ã§alÄ±ÅŸtÄ±rÄ±r ve sonucu LLM'e geri gÃ¶nderir.
        DÃ¶ndÃ¼rÃ¼len deÄŸer: (nihai_yanit_metni, eylem_plani_listesi_veya_hata_durumunda_None)
        """
        max_recursion = max_recursion_depth if max_recursion_depth is not None else self.config_data.get("LLM_FUNCTION_CALLING_MAX_RECURSION", 3)

        if current_recursion_depth >= max_recursion:
            return "âš ï¸ Fonksiyon Ã§aÄŸÄ±rma maksimum Ã¶zyineleme derinliÄŸine ulaÅŸtÄ±.", None

        payload: Dict[str, Any] = {
            "messages": messages,
            "model": model_name or self.default_model_name,
            "max_tokens": max_tokens or self.default_max_tokens,
            "temperature": temperature,
            "tools": [{"type": "function", "function": fd} for fd in self.aybar._build_agent_prompt_messages("","","","","")[1]] # Tool definitions EnhancedAybar'dan alÄ±nacak
            # YukarÄ±daki satÄ±rda _build_agent_prompt_messages'in ikinci elemanÄ± (functions_definition) alÄ±nmalÄ±.
            # Bu kÄ±sÄ±m EnhancedAybar'da _build_agent_prompt_messages'in nasÄ±l yapÄ±landÄ±rÄ±ldÄ±ÄŸÄ±na baÄŸlÄ±.
            # Åimdilik geÃ§ici olarak boÅŸ stringlerle Ã§aÄŸÄ±rÄ±yorum, bu dÃ¼zeltilmeli.
            # DoÄŸrusu: functions_definition = self.aybar.get_tool_definitions() gibi bir metod olmalÄ±.
            # VEYA EnhancedAybar._build_agent_prompt_messages'dan sadece tool_definitions alÄ±nmalÄ±.
            # GeÃ§ici Ã§Ã¶zÃ¼m:
            try:
                # Bu satÄ±r, _build_agent_prompt_messages'in ikinci elemanÄ±nÄ± (fonksiyon tanÄ±mlarÄ±) almayÄ± hedefler.
                # Ancak, bu Ã§aÄŸrÄ± ÅŸekli doÄŸru olmayabilir ve EnhancedAybar'da uygun bir arayÃ¼z gerektirir.
                # Åimdilik bu ÅŸekilde bÄ±rakÄ±yorum, EnhancedAybar refaktorasyonu sÄ±rasÄ±nda dÃ¼zeltilecek.
                # _build_agent_prompt_messages'in ilk argÃ¼manÄ± 'current_goal'
                # Bu Ã§aÄŸrÄ±, LLMManager'Ä±n EnhancedAybar'Ä±n iÃ§ yapÄ±sÄ±na fazla baÄŸÄ±mlÄ± olmasÄ±na neden oluyor.
                # Daha iyi bir Ã§Ã¶zÃ¼m, EnhancedAybar'Ä±n araÃ§ tanÄ±mlarÄ±nÄ± saÄŸlayan bir metodu olmasÄ±dÄ±r.
                # enhanced_aybar_instance = self.aybar
                # current_goal = enhanced_aybar_instance.cognitive_system.current_goal if enhanced_aybar_instance.cognitive_system else "Genel AmaÃ§"
                # last_observation = "Fonksiyon Ã§aÄŸrÄ±sÄ± iÃ§in araÃ§lar hazÄ±rlanÄ±yor." # Bu gÃ¶zlem metni Ã¶nemli deÄŸil, sadece prompt iÃ§in.
                # tool_definitions_messages = enhanced_aybar_instance._build_agent_prompt_messages(
                # current_goal or "Hedef Yok", last_observation, None, None, None
                # )
                # Burada messages[-1]['content'] iÃ§inde tool_calls'un nasÄ±l formatlandÄ±ÄŸÄ±na bakmak lazÄ±m.
                # Ya da LLMManager'a tool_definitions doÄŸrudan verilmeli.
                # Åimdilik, tools parametresini kullanarak bir yapÄ± oluÅŸturalÄ±m.
                tool_definitions = []
                import inspect
                for name, func_ref in tools.items():
                    docstring = inspect.getdoc(func_ref)
                    description = docstring.split('\n')[0] if docstring else "No description available."
                    sig = inspect.signature(func_ref)
                    param_keys = list(sig.parameters.keys())
                    actual_params = param_keys[1:] if param_keys and (param_keys[0] == 'aybar_instance' or param_keys[0] == 'self') else param_keys
                    params_info = {
                        param_name: {
                            "type": "string", # Basitlik iÃ§in ÅŸimdilik hepsi string
                            "description": f"Parameter {param_name}"
                        } for param_name in actual_params
                    }
                    tool_definitions.append({
                        "type": "function",
                        "function": {
                            "name": name,
                            "description": description,
                            "parameters": {
                                "type": "object",
                                "properties": params_info,
                                "required": [p for p in actual_params if sig.parameters[p].default == inspect.Parameter.empty]
                            }
                        }
                    })
                payload["tools"] = tool_definitions
                payload["tool_choice"] = "auto" # LLM'in aracÄ± seÃ§mesine izin ver

            except Exception as e:
                print(f"âš ï¸ AraÃ§ tanÄ±mlarÄ± oluÅŸturulurken hata: {e}. Fonksiyon Ã§aÄŸÄ±rma devre dÄ±ÅŸÄ± bÄ±rakÄ±labilir.")
                payload.pop("tools", None)
                payload.pop("tool_choice", None)


        try:
            response = requests.post(self.api_url, headers=self._get_headers(), json=payload, timeout=self.default_timeout)
            response.raise_for_status()
            response_data = response.json()

            if not response_data.get("choices"):
                return f"âš ï¸ LLM yanÄ±tÄ±nda 'choices' alanÄ± bulunamadÄ±: {str(response_data)[:200]}", None

            message = response_data["choices"][0].get("message", {})
            finish_reason = response_data["choices"][0].get("finish_reason", "")

            action_plan: List[Dict[str, Any]] = []

            if message.get("tool_calls"):
                print(f"ğŸ› ï¸ LLM araÃ§ kullanmak istiyor: {message['tool_calls']}")
                # Etik DeÄŸerlendirme (EÄŸer aybar Ã¶rneÄŸi varsa ve etik sistem aktifse)
                if hasattr(self.aybar, 'ethical_framework'):
                    is_ethical, justification = self.aybar.ethical_framework.evaluate_action(
                        {"action": "tool_calls", "details": message["tool_calls"]},
                        {"messages": messages}
                    )
                    if not is_ethical:
                        print(f"âš–ï¸ Etik Ä°hlal: {justification}")
                        # Etik olmayan bir aracÄ± Ã§aÄŸÄ±rmak yerine bir dÃ¼ÅŸÃ¼nce dÃ¶ndÃ¼r
                        messages.append({"role": "assistant", "content": None, "tool_calls": message["tool_calls"]}) # LLM'in Ã§aÄŸrÄ±sÄ±nÄ± ekle
                        messages.append({
                            "role": "tool",
                            "tool_call_id": message["tool_calls"][0]["id"], # Ä°lk Ã§aÄŸrÄ±nÄ±n ID'si
                            "name": message["tool_calls"][0]["function"]["name"],
                            "content": f"{{\"error\": \"Etik dÄ±ÅŸÄ± eylem engellendi: {justification}\"}}"
                        })
                        # LLM'e durumu bildirip devam etmesini iste
                        return self.ask_llm_with_function_calling(messages, tools, model_name, max_tokens, temperature, max_recursion, current_recursion_depth + 1)

                # AraÃ§larÄ± Ã§alÄ±ÅŸtÄ±r
                for tool_call in message["tool_calls"]:
                    function_name = tool_call.get("function", {}).get("name")

                    try:
                        function_args_str = tool_call.get("function", {}).get("arguments", "{}")
                        function_args = json.loads(function_args_str) if isinstance(function_args_str, str) else function_args_str
                    except json.JSONDecodeError:
                        print(f"âš ï¸ Fonksiyon argÃ¼manlarÄ± JSON parse edilemedi: {function_args_str}")
                        function_args = {"error": "Invalid JSON arguments"}


                    if function_name in tools:
                        try:
                            function_to_call = tools[function_name]
                            # Araca Aybar Ã¶rneÄŸini ve diÄŸer parametreleri ilet
                            # Not: tools.py iÃ§indeki fonksiyonlar (self.aybar, **kwargs) alacak ÅŸekilde tasarlanmalÄ±
                            print(f"â–¶ï¸ AraÃ§ Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor: {function_name} args: {function_args}")
                            # BazÄ± araÃ§lar `aybar_instance` gerektirebilir.
                            # Bu, araÃ§larÄ±n nasÄ±l tanÄ±mlandÄ±ÄŸÄ±na baÄŸlÄ±.
                            # EÄŸer araÃ§lar `(aybar_instance, **kwargs)` alÄ±yorsa:
                            if inspect.signature(function_to_call).parameters.get('aybar_instance'):
                                tool_response = function_to_call(aybar_instance=self.aybar, **function_args)
                            else: # Sadece kwargs alÄ±yorsa:
                                tool_response = function_to_call(**function_args)

                            print(f"â—€ï¸ AraÃ§ yanÄ±tÄ± ({function_name}): {str(tool_response)[:200]}...")

                            # Eylem planÄ±na bu adÄ±mÄ± ekle (gerÃ§ek yanÄ±t yerine Ã§aÄŸrÄ±yÄ±)
                            action_plan.append({
                                "action": function_name,
                                "parameters": function_args,
                                "thought": message.get("content") or f"{function_name} aracÄ± Ã§aÄŸrÄ±ldÄ±."
                                           # LLM bazen tool_calls ile birlikte content:null dÃ¶nebilir.
                            })

                            # LLM'e gÃ¶ndermek iÃ§in mesaj listesini gÃ¼ncelle
                            messages.append({"role": "assistant", "content": message.get("content"), "tool_calls": [tool_call]})
                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call["id"],
                                "name": function_name,
                                "content": json.dumps({"result": tool_response}, ensure_ascii=False)
                                           if isinstance(tool_response, (dict, list))
                                           else json.dumps({"result": str(tool_response)}, ensure_ascii=False)
                            })

                        except Exception as e:
                            print(f"âŒ AraÃ§ Ã§alÄ±ÅŸtÄ±rÄ±lÄ±rken hata ({function_name}): {e}")
                            messages.append({"role": "assistant", "content": None, "tool_calls": [tool_call]})
                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call["id"],
                                "name": function_name,
                                "content": json.dumps({"error": str(e)}, ensure_ascii=False)
                            })
                            action_plan.append({
                                "action": "ERROR_IN_TOOL_CALL",
                                "parameters": {"tool_name": function_name, "error": str(e)},
                                "thought": f"{function_name} aracÄ± Ã§alÄ±ÅŸtÄ±rÄ±lÄ±rken hata oluÅŸtu."
                            })


                    else: # Bilinmeyen fonksiyon
                        print(f"âš ï¸ Bilinmeyen fonksiyon Ã§aÄŸrÄ±sÄ±: {function_name}")
                        messages.append({"role": "assistant", "content": None, "tool_calls": [tool_call]})
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call["id"],
                            "name": function_name,
                            "content": json.dumps({"error": f"Fonksiyon '{function_name}' bulunamadÄ±."}, ensure_ascii=False)
                        })
                        action_plan.append({
                            "action": "UNKNOWN_TOOL",
                            "parameters": {"tool_name": function_name},
                            "thought": f"Bilinmeyen bir araÃ§ ({function_name}) Ã§aÄŸrÄ±lmaya Ã§alÄ±ÅŸÄ±ldÄ±."
                        })

                # AraÃ§ yanÄ±tlarÄ±yla birlikte LLM'i tekrar Ã§aÄŸÄ±r
                return self.ask_llm_with_function_calling(messages, tools, model_name, max_tokens, temperature, max_recursion, current_recursion_depth + 1)

            else: # Fonksiyon Ã§aÄŸrÄ±sÄ± yok, doÄŸrudan yanÄ±t
                final_response_text = message.get("content", "").strip()
                if not final_response_text and not action_plan: # EÄŸer iÃ§erik boÅŸsa ve plan da yoksa
                    final_response_text = "(LLM sessiz kaldÄ± veya sadece araÃ§ Ã§aÄŸÄ±rmayÄ± dÃ¼ÅŸÃ¼ndÃ¼ ama yapmadÄ±)"

                # EÄŸer LLM doÄŸrudan bir dÃ¼ÅŸÃ¼nce/monolog dÃ¶ndÃ¼rdÃ¼yse, bunu bir eylem olarak paketle
                if final_response_text and not action_plan:
                    action_plan.append({
                        "action": "CONTINUE_INTERNAL_MONOLOGUE",
                        "thought": final_response_text
                    })
                elif not final_response_text and not action_plan: # Her ikisi de boÅŸsa
                     action_plan.append({
                        "action": "CONTINUE_INTERNAL_MONOLOGUE",
                        "thought": "(DÃ¼ÅŸÃ¼nce Ã¼retilmedi)"
                    })

                return final_response_text, action_plan

        except requests.exceptions.RequestException as e:
            self._log_llm_error(f"Function Calling RequestException: {e}", payload)
            self._last_error_time = time.time()
            return f"âš ï¸ LLM BaÄŸlantÄ± HatasÄ± (Fonksiyon Ã‡aÄŸÄ±rma): {e}", None
        except json.JSONDecodeError as e:
            self._log_llm_error(f"Function Calling JSONDecodeError: Sunucu yanÄ±tÄ± JSON deÄŸil. YanÄ±t: {response.text[:200]}", payload)
            self._last_error_time = time.time()
            return f"âš ï¸ LLM YanÄ±t HatasÄ± (Fonksiyon Ã‡aÄŸÄ±rma): Sunucudan gelen yanÄ±t JSON formatÄ±nda deÄŸil.", None
        except Exception as e:
            self._log_llm_error(f"Function Calling Genel Hata: {type(e).__name__} - {e}", payload)
            self._last_error_time = time.time()
            return f"âš ï¸ LLM Genel HatasÄ± (Fonksiyon Ã‡aÄŸÄ±rma): {type(e).__name__} - {e}", None

    def _log_llm_error(self, error_message: str, payload: Optional[Dict] = None):
        """LLM hatalarÄ±nÄ± loglar (ÅŸimdilik sadece print ediyor)."""
        # TODO: Daha geliÅŸmiÅŸ loglama (dosyaya, veritabanÄ±na vb.)
        print(f"âŒ LLM_MANAGER_ERROR: {error_message}")
        if payload:
            # Hassas bilgileri (API anahtarÄ± gibi) loglamadan Ã¶nce payload'Ä± temizle
            safe_payload = {k: v for k, v in payload.items() if k not in ["api_key", "headers"]} # Ã–rnek
            print(f"    Payload (gÃ¼venli): {str(safe_payload)[:500]}...") # Payload'Ä±n bir kÄ±smÄ±nÄ± logla

    def sanitize_llm_output(self, text: str) -> str:
        """LLM Ã§Ä±ktÄ±sÄ±ndaki istenmeyen prompt parÃ§alarÄ±nÄ± ve kod bloklarÄ±nÄ± temizler."""
        if not isinstance(text, str): # Girdi string deÄŸilse stringe Ã§evir
            text = str(text)
        text = re.sub(r'---.*?---', '', text, flags=re.DOTALL) # --- HEADER --- gibi yapÄ±larÄ± temizle
        text = re.sub(r'```.*?```', '', text, flags=re.DOTALL) # ``` ... ``` ile Ã§evrili kod bloklarÄ±nÄ± temizle
        # Aybar'Ä±n kendi kendine sorduÄŸu veya tekrarladÄ±ÄŸÄ± bazÄ± kalÄ±plarÄ± temizle
        lines = text.split('\n')
        cleaned_lines = [line for line in lines if not (
            line.strip().lower().startswith("ben aybar") or
            line.strip().lower().startswith("benim iÃ§in soru") or
            line.strip().lower().startswith("sen aybar") # Bazen LLM prompt'u tekrar eder
        )]
        return "\n".join(cleaned_lines).strip()

from typing import Union # Union importu dosya baÅŸÄ±na taÅŸÄ±ndÄ±
